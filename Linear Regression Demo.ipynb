{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“This scientific way of analysing data or extracting knowledge out of data is called Data science.”\n",
    "\n",
    "                                                        OR\n",
    "\n",
    "“Data science is all about making sense out of the data or extracting the knowledge from the data using data science techniques.”\n",
    "\n",
    "                                                        OR\n",
    "                                                        \n",
    "“Data science, also known as data-driven science, is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either it is structured data, semi-structure data or unstructured data.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.1.png\"  style=\"width:50%;height:50%\" align=\"middle\" alt=\"Figure 1.1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* What is Natural Language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is Natural Language Processing (NLP)?\n",
    "    * Natural language processing is the ability of computational technologies and/or computational linguistics to process human natural language.\n",
    "\n",
    "    * Natural language processing is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages\n",
    "\n",
    "    * Natural language processing can be defined as the automatic (or semi-automatic) processing of human natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.3.png\" align=\"middle\" style=\"width:70%;height:70%\" alt=\"Figure 1.3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.4.png\" align=\"middle\"style=\"width:70%;height:70%\" alt=\"Figure 1.4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development cycle of NLP applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.5.png\" align=\"middle\" style=\"width:400px;height:600px\" alt=\"Figure 1.5\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* In 1959, a researcher named <b> Arthur Samuel </b> gave computers the ability to learn without being explicitly programmed. He evolved this concept of ML from the study of pattern recognition and computational learning theory in AI. \n",
    "\n",
    "\n",
    "* In 1997, <b>Tom Mitchell</b> gave us an accurate definition that has been useful to those who can understand basic math. The definition of ML as per Tom Mitchell is: \n",
    "    * computer program is said to learn from <b> experience E </b> with respect to some <b>task T</b> and some <b>performance measure P</b>, if its <b>performance on T, as measured by P, improves with experience E</b>.\n",
    "\n",
    "\n",
    "* Let's link the preceding definition with our previous example. \n",
    "    * To <b>identify a license plate is called task T </b>. You will run some <b>ML programs using examples of license plates called experience E</b>, and if it <b>successfully learns</b>, then it can <b>predict the next unseen license plate that is called performance measure P.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.6.jpg\" align=\"middle\" alt=\"Figure 1.6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.7.png\" style=\"width:70%;height:70%\" align=\"middle\" alt=\"Figure 1.7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's time for some basic hands on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I'm using a very small dataset of <b>student test scores</b> and <b>the amount of hours they studied.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.8.png\" style=\"width:50%;height:50%\" align=\"middle\" alt=\"Figure 1.8\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intuitively, we know that there must be a relationship right? The more you study, the better your test scores should be. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We're going to use <b><i>linear regression</i></b> to prove this relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps we are going to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data\n",
    "2. Initialization of the parameters\n",
    "3. Define Linear Equation\n",
    "4. Define and understand <b>Sum of Squared Error value</b> and <b>equation</b>\n",
    "5. Calculate <b>Gradient Descent</b> in order to get the line of best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Gradient Descent based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%20%201.9.gif\" style=\"width:100%;height:100%\" align=\"middle\" alt=\"Figure 1.9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.10.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.10\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Squared Error Value and Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.11.png\" style=\"width:50%;height:50%\" align=\"middle\" alt=\"Figure 1.11\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared distance equation in statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.12.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.12\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of squared distances formula (to calculate our error) linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.13.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.13\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.14.png\" style=\"width:80%;height:80%\" align=\"middle\" alt=\"Figure 1.14\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.15.png\" style=\"width:60%;height:60%\" align=\"middle\" alt=\"Figure 1.15\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.16.png\" style=\"width:70%;height:70%\" align=\"middle\" alt=\"Figure 1.16\">\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivative with respect to b and m (to perform gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jalajthanaki/Linear-Regression-Workshop/master/image/Figure%201.17.png\" style=\"width:40%;height:40%\" align=\"middle\" alt=\"Figure 1.17\">\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "# here we are calculating the sum of squared error by using the equation which we have seen.\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # Here we are coding up out partial derivatives equations and\n",
    "        # generate the updated value for m and b to get the local minima\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    # we are multiplying the b_gradient and m_gradient with learningrate\n",
    "    # so it is important to choose ideal learning rate if we make it to high then our model learn nothing\n",
    "    # if we make it to small then our training is to slow and there are the chances of over fitting\n",
    "    # so learning rate is important hyper parameter.\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        # we are using step_gradient function to calculate the actual partial derivatives for error function\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    # Step 1 : Read data\n",
    "\n",
    "    # genfromtext is used to read out data from data.csv file.\n",
    "    points = genfromtxt(\"./data/data.csv\", delimiter=\",\")\n",
    "\n",
    "    # Step2 : Define certain hyperparameters\n",
    "\n",
    "    # how fast our model will converge means how fast we will get the line of best fit.\n",
    "    # Converge means how fast our ML model get the optimal line of best fit.\n",
    "    learning_rate = 0.0001\n",
    "    # Here we need to draw the line which is best fit for our data.\n",
    "    # so we are using y = mx + b ( x and y are points; m is slop; b is the y intercept)\n",
    "    # for initial y-intercept guess\n",
    "    initial_b = 0\n",
    "    # initial slope guess\n",
    "    initial_m = 0\n",
    "    # How much do you want to train the model?\n",
    "    # Here data set is small so we iterate this model for 1000 times.\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    # Step 3 - print the values of b, m and all function which calculate gradient descent and errors\n",
    "    # Here we are printing the initial values of b, m and error.\n",
    "    # As well as there is the function compute_error_for_line_given_points()\n",
    "    # which compute the errors for given point\n",
    "    print \"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m,\n",
    "                                                                              compute_error_for_line_given_points(initial_b, initial_m, points))\n",
    "    print \"Running...\"\n",
    "\n",
    "    # By using this gradient_descent_runner() function we will actually calculate gradient descent\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "\n",
    "    # Here we are printing the values of b, m and error after getting the line of best fit for the given dataset.\n",
    "    print \"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
